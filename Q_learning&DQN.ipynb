{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjdA1ktcd213"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import gym\n",
        "from gym.envs.toy_text import blackjack\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import pdb\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "GPU_indx = 0\n",
        "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu6JifdFd0HC"
      },
      "source": [
        "Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AQuYsRxeLvO",
        "outputId": "b3a2326e-7e06-4116-b5c6-784681c3f76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space: Discrete(2). \n",
            "\n",
            "=> (1 = Hit, 0 = Bust)\n",
            "\n",
            "-----------------------------------------------\n",
            "\n",
            "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2)) \n",
            "\n",
            "=> (Player hand sum, Dealer card, Usable ace)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Blackjack-v1',new_step_api=True)\n",
        "env.reset()\n",
        "print(f'Action space: {env.action_space}. \\n\\n=> (1 = Hit, 0 = Bust)\\n')\n",
        "print('-----------------------------------------------\\n')\n",
        "print(f'Observation space: {env.observation_space} \\n\\n=> (Player hand sum, Dealer card, Usable ace)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHbROl7Kd0cZ"
      },
      "source": [
        "Random test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GdYlCwIdk6g",
        "outputId": "d56d375d-73fc-4f05-88c6-67c9c024360e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward over 1000 episodes with random policy: -0.374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "def test_agent_random(env, num_episodes=1000):\n",
        "    total_rewards = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()  # 初始化环境，获取初始状态\n",
        "        state = (state[0], state[1], state[2])  # 只取出需要的部分：player_sum, dealer_show, usable_ace\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # 随机选择动作\n",
        "            action = random.choice([0, 1])  # 假设动作空间是 [0, 1]\n",
        "\n",
        "            # 与环境互动，获得下一个状态和奖励\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            next_state = (next_state[0], next_state[1], next_state[2])  # 只取出需要的部分\n",
        "\n",
        "            # 更新总奖励\n",
        "            episode_reward += reward\n",
        "\n",
        "            # 更新状态\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards += episode_reward\n",
        "\n",
        "    average_reward = total_rewards / num_episodes\n",
        "    return average_reward\n",
        "\n",
        "average_reward_random = test_agent_random(env, num_episodes=1000)\n",
        "print(f\"Average reward over 1000 episodes with random policy: {average_reward_random}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx7LqORF5_mD"
      },
      "source": [
        "Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEN1sISEFGTj",
        "outputId": "ef5dda34-4765-43a2-9e9e-e6d8ec7c0f6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Blackjack-v1',new_step_api=True)\n",
        "Q = {}\n",
        "\n",
        "for player_sum in range(4, 33):  # 玩家点数从4到32\n",
        "    for dealer_show in range(1, 11):  # 庄家明牌从1到10\n",
        "        for usable_ace in [True, False]:  # 是否有软牌\n",
        "            Q[(player_sum, dealer_show, usable_ace)] = [0, 0]  # 0: hit, 1: stick\n",
        "\n",
        "# Q-learning 参数\n",
        "alpha = 0.1  # 学习率\n",
        "gamma = 0.99  # 折扣因子\n",
        "epsilon = 0.1  # 探索率\n",
        "episodes = 1000  # 训练的回合数\n",
        "\n",
        "# 动作选择策略（epsilon-greedy）\n",
        "def epsilon_greedy(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # 随机选择动作（探索）\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        # 选择Q值最大的动作（利用）\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "# Q-learning 训练过程\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()  # 初始化环境，获取初始状态\n",
        "    state = (state[0], state[1], state[2])  # 只取出需要的部分：player_sum, dealer_show, usable_ace\n",
        "    done = False\n",
        "    while not done:\n",
        "        # 使用 epsilon-greedy 策略选择动作\n",
        "        action = epsilon_greedy(state)\n",
        "\n",
        "        # 与环境互动，获得下一个状态和奖励\n",
        "        next_state, reward,terminated, truncated, _ = env.step(action)\n",
        "        next_state = (next_state[0], next_state[1], next_state[2])  # 只取出需要的部分\n",
        "\n",
        "        # 如果 next_state 超出 Q 表的初始化范围，则返回到初始状态，避免错误\n",
        "        if next_state[0] > 32:\n",
        "            next_state = (32, next_state[1], next_state[2])  # 处理 player_sum 大于 21 的情况\n",
        "\n",
        "        # 更新 Q 值\n",
        "        current_q = Q[state][action]\n",
        "        max_next_q = max(Q[next_state])\n",
        "\n",
        "        # Q-learning 更新公式\n",
        "        Q[state][action] = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
        "\n",
        "        # 更新状态\n",
        "        state = next_state\n",
        "\n",
        "    # 每1000个回合打印一次进度\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}/{episodes}\")\n",
        "\n",
        "print(\"训练完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjZdHeQRf7o0"
      },
      "outputs": [],
      "source": [
        "def test_agent(env, Q, num_episodes=1000):\n",
        "    total_rewards = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()  # 初始化环境，获取初始状态\n",
        "        state = (state[0], state[1], state[2])  # 只取出需要的部分：player_sum, dealer_show, usable_ace\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # 使用 Q 表选择动作（利用）\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "            # 与环境互动，获得下一个状态和奖励\n",
        "            next_state, reward, done, _= env.step(action)\n",
        "            next_state = (next_state[0], next_state[1], next_state[2])  # 只取出需要的部分\n",
        "\n",
        "            # 更新总奖励\n",
        "            episode_reward += reward\n",
        "\n",
        "            # 更新状态\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards += episode_reward\n",
        "\n",
        "    average_reward = total_rewards / num_episodes\n",
        "    return average_reward\n",
        "\n",
        "average_reward = test_agent(env, Q, num_episodes=1000)\n",
        "print(f\"Average reward over 1000 episodes: {average_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udSoyXiDip9k"
      },
      "source": [
        "Deep Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjcLsoL0GqLt",
        "outputId": "d960bc66-e8f3-46bd-a92e-3456af6c01ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward over 1000 episodes: -0.15\n"
          ]
        }
      ],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "g-ifWhZygFcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        # 定义 Q 网络和目标网络\n",
        "        self.policy_net = DQN(state_dim, action_dim)\n",
        "        self.target_net = DQN(state_dim, action_dim)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        # 定义优化器和损失函数\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # 经验回放缓冲区\n",
        "        self.buffer = ReplayBuffer(10000)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)  # 随机动作\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state)\n",
        "                q_values = self.policy_net(state)\n",
        "                return torch.argmax(q_values).item()  # 选择 Q 值最大的动作\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        # 从缓冲区中采样\n",
        "        state, action, reward, next_state, done = self.buffer.sample(batch_size)\n",
        "        state = torch.FloatTensor(state)\n",
        "        action = torch.LongTensor(action)\n",
        "        reward = torch.FloatTensor(reward)\n",
        "        next_state = torch.FloatTensor(next_state)\n",
        "        done = torch.FloatTensor(done)\n",
        "\n",
        "        # 计算当前 Q 值\n",
        "        current_q = self.policy_net(state).gather(1, action.unsqueeze(1))\n",
        "\n",
        "        # 计算目标 Q 值\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_state).max(1)[0]\n",
        "            target_q = reward + (1 - done) * self.gamma * next_q\n",
        "\n",
        "        # 计算损失并更新网络\n",
        "        loss = self.loss_fn(current_q.squeeze(), target_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 更新 epsilon\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ],
      "metadata": {
        "id": "quVxWeEBgIwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v1')\n",
        "state_dim = 3  # player_sum, dealer_show, usable_ace\n",
        "action_dim = 2  # 0: stick, 1: hit\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "batch_size = 64\n",
        "num_episodes = 1000\n",
        "update_target_every = 10\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # 选择动作\n",
        "        action = agent.select_action(state)\n",
        "\n",
        "        # 执行动作\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # 存储经验\n",
        "        agent.buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        # 更新状态\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # 更新网络\n",
        "        agent.update(batch_size)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 更新目标网络\n",
        "    if episode % update_target_every == 0:\n",
        "        agent.update_target_net()\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")"
      ],
      "metadata": {
        "id": "RLUQ_8vygOgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(env, agent, num_episodes=100):\n",
        "    total_rewards = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_rewards += episode_reward\n",
        "        print(f\"Test Episode: {episode + 1}, Reward: {episode_reward}\")\n",
        "\n",
        "    average_reward = total_rewards / num_episodes\n",
        "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
        "\n",
        "test_agent(env, agent)"
      ],
      "metadata": {
        "id": "iltf0IJWgLhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}